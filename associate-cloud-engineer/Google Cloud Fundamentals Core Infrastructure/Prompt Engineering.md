Large language models refer to large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes.

In this context, large refers to: The size of the training dataset, which can sometimes be at the petabyte scale And the number of parameters.

Parameters are the memories and knowledge that the machine has learned during model training.
They determine the ability of a model to solve a problem, such as predicting text, and can reach billions or even trillions in size.

**How are LLM's trained ?**
LLMs are trained through **pre-training**, where they are fed massive datasets of text, images, and code to learn language structure and patterns. This process allows them to predict the most probable correct response to a prompt, acting like advanced autocomplete. However, they can produce incorrect or nonsensical outputs called **hallucinations** because they are limited to their training data, lack real-time information, assume the prompt is true, and cannot ask for clarification. Hallucinations can stem from insufficient or flawed training data, or a lack of context and constraints in the prompt.